{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章 提升方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．提升方法是将弱学习算法提升为强学习算法的统计学习方法。在分类学习中，提升方法通过反复修改训练数据的权值分布，构建一系列基本分类器（弱分类器），并将这些基本分类器线性组合，构成一个强分类器。代表性的提升方法是AdaBoost算法。\n",
    "\n",
    "AdaBoost模型是弱分类器的线性组合：\n",
    "\n",
    "$$f(x)=\\sum_{m=1}^{M} \\alpha_{m} G_{m}(x)$$\n",
    "\n",
    "2．AdaBoost算法的特点是通过迭代每次学习一个基本分类器。每次迭代中，提高那些被前一轮分类器错误分类数据的权值，而降低那些被正确分类的数据的权值。最后，AdaBoost将基本分类器的线性组合作为强分类器，其中给分类误差率小的基本分类器以大的权值，给分类误差率大的基本分类器以小的权值。\n",
    "\n",
    "3．AdaBoost的训练误差分析表明，AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率，这说明了它作为提升方法的有效性。\n",
    "\n",
    "4．AdaBoost算法的一个解释是该算法实际是前向分步算法的一个实现。在这个方法里，模型是加法模型，损失函数是指数损失，算法是前向分步算法。\n",
    "每一步中极小化损失函数\n",
    "\n",
    "$$\\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right)$$\n",
    "\n",
    "得 到 参 数$\\beta_{m}, \\gamma_{m}$。\n",
    "\n",
    "5．提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中最有效的方法之一。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Boost\n",
    "\n",
    "“装袋”（bagging）和“提升”（boost）是构建组合模型的两种最主要的方法，所谓的组合模型是由多个基本模型构成的模型，组合模型的预测效果往往比任意一个基本模型的效果都要好。\n",
    "\n",
    "- 装袋：每个基本模型由从总体样本中随机抽样得到的不同数据集进行训练得到，通过重抽样得到不同训练数据集的过程称为装袋。\n",
    "\n",
    "- 提升：每个基本模型训练时的数据集采用不同权重，针对上一个基本模型分类错误的样本增加权重，使得新的模型重点关注误分类样本\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "AdaBoost是AdaptiveBoost的缩写，表明该算法是具有适应性的提升算法。\n",
    "\n",
    "算法的步骤如下：\n",
    "\n",
    "1）给每个训练样本（$x_{1},x_{2},….,x_{N}$）分配权重，初始权重$w_{1}$均为1/N。\n",
    "\n",
    "2）针对带有权值的样本进行训练，得到模型$G_m$（初始模型为G1）。\n",
    "\n",
    "3）计算模型$G_m$的误分率$e_m=\\sum_{i=1}^Nw_iI(y_i\\not= G_m(x_i))$\n",
    "\n",
    "4）计算模型$G_m$的系数$\\alpha_m=0.5\\log[(1-e_m)/e_m]$\n",
    "\n",
    "5）根据误分率e和当前权重向量$w_m$更新权重向量$w_{m+1}$。\n",
    "\n",
    "6）计算组合模型$f(x)=\\sum_{m=1}^M\\alpha_mG_m(x_i)$的误分率。\n",
    "\n",
    "7）当组合模型的误分率或迭代次数低于一定阈值，停止迭代；否则，回到步骤2）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #导库\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection  import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "def create_data(): #创建数据集\n",
    "    iris = load_iris() #加载\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[:100, [0, 1, -1]])\n",
    "    for i in range(len(data)):\n",
    "        if data[i,-1] == 0:\n",
    "            data[i,-1] = -1\n",
    "    # print(data)\n",
    "    return data[:,:2], data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16e52055660>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGgCAYAAAB45mdaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAzUUlEQVR4nO3df3RU9Z3/8ddkQhLQJAhrkuFrhPijaMhqkegaKliLIqB096yn292DIlvlNApapZ7F2FaXbW106+5Sj10sOeopZlt2T4MuHClCVxNoDUd+pQWDlFMT4GBiVGwCIolJ7veP6SSZJDOZH3fmfubO83HOHM/c+czM+77vlXnn/vi8PZZlWQIAAHBIhtMBAACA9EYxAgAAHEUxAgAAHEUxAgAAHEUxAgAAHEUxAgAAHEUxAgAAHEUxAgAAHEUxAgAAHEUxAgAAHBVXMVJdXS2Px6OHHnoo5Jj6+np5PJ4Rj3fffTeerwYAAC6RGesb9+zZo/Xr1+uqq66KaPyRI0eUl5c38PzCCy+M+Lv6+/v1/vvvKzc3Vx6PJ+pYAQBA8lmWpdOnT2vKlCnKyAh9/COmYuTMmTNasmSJampq9IMf/CCi9xQUFGjixImxfJ3ef/99FRcXx/ReAADgrBMnTuiiiy4K+XpMxciKFSt022236eabb464GJk5c6bOnTun0tJSffe739VNN90Ucmx3d7e6u7sHngcaC584cSLo6AoAADBXV1eXiouLlZubG3Zc1MXIxo0btX//fu3Zsyei8T6fT+vXr9esWbPU3d2tl19+WfPmzVN9fb3mzp076nuqq6u1Zs2aEcvz8vIoRgAASDFjXWLhsQKHHSJw4sQJlZeXa/v27br66qslSV/+8pf1xS9+UWvXro04qMWLF8vj8Wjz5s2jvj78yEigsurs7KQYAQAgRXR1dSk/P3/M3++o7qbZt2+fOjo6NGvWLGVmZiozM1MNDQ169tlnlZmZqb6+vog+5/rrr9fRo0dDvp6dnT1wFISjIQAAuFtUp2nmzZungwcPBi37x3/8R11xxRVavXq1vF5vRJ9z4MAB+Xy+aL4aAAC4VFTFSG5ursrKyoKWnXfeeZo8efLA8qqqKp08eVIbNmyQJK1du1bTpk3TjBkz1NPTo9raWtXV1amurs6mVQAAwEyWZam3tzfiMwepxuv1KjMzM+5pN2KeZySUtrY2HT9+fOB5T0+PHnnkEZ08eVLjx4/XjBkz9Nprr2nRokV2fzUAAMbo6elRW1ubzp4963QoCTVhwgT5fD5lZWXF/BlRXcDqlEgvgAEAwAT9/f06evSovF6vLrzwQmVlZblu0k7LstTT06MPP/xQfX19uvzyy0dMbBbp77ftR0YAAEh3PT096u/vV3FxsSZMmOB0OAkzfvx4jRs3TseOHVNPT49ycnJi+hwa5QEAkCDhpkB3CzvWkSMjAIzV12/p7ZZT6jh9TgW5ObquZJK8Ge461A2AYgSAobYdatOaLc1q6zw3sMyXn6MnFpdqQRlTAwBu4v7jRwBSzrZDbbqvdn9QISJJ7Z3ndF/tfm071OZQZEB6+M///E+VlJQoJydHs2bN0q5duxL6fRQjAIzS129pzZZmjXabX2DZmi3N6us3/kZAICX993//tx566CF95zvf0YEDBzRnzhwtXLgwaNoOu1GMADDK2y2nRhwRGcqS1NZ5Tm+3nEpeUIBD+votNf7xY/1v00k1/vHjpBTh//7v/6577rlH9957r6688kqtXbtWxcXFWrduXcK+k2tGABil43ToQiSWcUCqcuK6qZ6eHu3bt0+PPvpo0PL58+frrbfeSsh3ShwZAWCYgtzI5imIdByQipy6buqjjz5SX1+fCgsLg5YXFhaqvb09Id8pUYwAMMx1JZPky89RqBt4PfL/dXhdyaRkhgUkjQnXTQ2fLdayrITOIEsxAsAo3gyPnlhcKkkjCpLA8ycWlzLfCFzLyeum/uIv/kJer3fEUZCOjo4RR0vsRDECwDgLynxad+c1KsoPPhVTlJ+jdXdewzwjcDUnr5vKysrSrFmztGPHjqDlO3bs0OzZs23/vgAuYAVgpAVlPt1SWsQMrEg7Tl83tWrVKt11110qLy9XRUWF1q9fr+PHj6uysjIh3ydRjAAwmDfDo4pLJzsdBpBUgeum2jvPjXrdiEf+o4SJum7q61//uj7++GP9y7/8i9ra2lRWVqatW7dq6tSpCfk+idM0AAAYxYTrpu6//361traqu7tb+/bt09y5cxP2XRLFCAAAxkm366Y4TQMAgIHS6bopihEAAAyVLtdNcZoGAAA4imIEAAA4imIEAAA4imIEAAA4imIEAAA4imIEAAA4imIEAAA4imIEAAAM2LlzpxYvXqwpU6bI4/Ho1VdfTfh3UowAAGCq/j6pZZd08Jf+//b3JfwrP/30U1199dV67rnnEv5dAczACgCAiZo3S9tWS13vDy7LmyIteFoq/WrCvnbhwoVauHBhwj5/NBwZAQDANM2bpf9ZGlyISFJXm39582Zn4koQihEAAEzS3+c/IiJrlBf/vGzbo0k5ZZMsFCMARtXXb6nxjx/rf5tOqvGPH6uvf7R/GAHY7thbI4+IBLGkrpP+cS7BNSMARth2qE1rtjSrrfPcwDJffo6eWFyqBWU+ByMD0sCZD+wdlwI4MgIgyLZDbbqvdn9QISJJ7Z3ndF/tfm071OZQZECaOL/Q3nEpgGIEwIC+fktrtjSHO1OtNVuaOWUDJNLU2f67ZuQJMcAj5f0//7gEOHPmjJqamtTU1CRJamlpUVNTk44fP56Q75MoRgAM8XbLqRFHRIayJLV1ntPbLaeSFxSQbjK8/tt3JY0sSP78fMFT/nEJsHfvXs2cOVMzZ86UJK1atUozZ87U448/npDvk7hmBMAQHadDFyKxjAMQo9KvSn+3IcQ8I08ldJ6RL3/5y7Ks5B79pBgBMKAgN8fWcQDiUPpV6Yrb/HfNnPnAf43I1NkJOyLiJIoRAAOuK5kkX36O2jvPjXrdiEdSUX6OriuZlOzQgPSU4ZVK5jgdRcJxzQiAAd4Mj55YXCop5JlqPbG4VN6MUBfWAUD0KEYABFlQ5tO6O69RUX7wqZii/Bytu/Ma5hkBYDtO0wAYYUGZT7eUFuntllPqOH1OBbn+UzMcEQGQCBQjAEblzfCo4tLJTocBpLRk35XiBDvWkdM0AADYbNy4cZKks2fPOhxJ4gXWMbDOseDICAAANvN6vZo4caI6OjokSRMmTJDH467TnJZl6ezZs+ro6NDEiRPl9cZ+yzHFCGCzvn6Lay0AqKioSJIGChK3mjhx4sC6xopiBLAR3W4BBHg8Hvl8PhUUFOjzzz93OpyEGDduXFxHRAIoRgCbBLrdDr+UK9DtlttigfTk9Xpt+cF2My5gBWxAt1sAiB3FCGADut0CQOwoRgAb0O0WAGJHMQLYgG63ABA7ihHABoFut6Fu4PXIf1cN3W4BYCSKEcAGdLsFgNhRjAA2odstAMSGeUYAG9HtFgCiRzEC2IxutwAQHU7TAAAAR1GMAAAAR3GaBoCr0UUZMF9cR0aqq6vl8Xj00EMPhR3X0NCgWbNmKScnR5dccomef/75eL4WACKy7VCbbnj6Df1DzW59a2OT/qFmt254+g1tO9TmdGgAhoi5GNmzZ4/Wr1+vq666Kuy4lpYWLVq0SHPmzNGBAwf02GOP6cEHH1RdXV2sXw0AYwp0UR7eMyjQRZmCBDBHTMXImTNntGTJEtXU1OiCCy4IO/b555/XxRdfrLVr1+rKK6/Uvffeq2984xt65plnYgoYAMZCF2UgtcRUjKxYsUK33Xabbr755jHHNjY2av78+UHLbr31Vu3du1eff/75qO/p7u5WV1dX0AMAIkUXZSC1RF2MbNy4Ufv371d1dXVE49vb21VYWBi0rLCwUL29vfroo49GfU91dbXy8/MHHsXFxdGGCSCN0UUZSC1RFSMnTpzQt771LdXW1ionJ/Luox5P8JXrlmWNujygqqpKnZ2dA48TJ05EEyaANEcXZSC1RHVr7759+9TR0aFZs2YNLOvr69POnTv13HPPqbu7W16vN+g9RUVFam9vD1rW0dGhzMxMTZ48+iyV2dnZys7OjiY0ABgQ6KLc3nlu1OtGPPL3DKKLMmCGqI6MzJs3TwcPHlRTU9PAo7y8XEuWLFFTU9OIQkSSKioqtGPHjqBl27dvV3l5ucaNGxdf9AAwCrooA6klqmIkNzdXZWVlQY/zzjtPkydPVllZmST/KZalS5cOvKeyslLHjh3TqlWrdPjwYb344ot64YUX9Mgjj9i7JgAwBF2UgdRh+wysbW1tOn78+MDzkpISbd26VQ8//LB+8pOfaMqUKXr22Wd1xx132P3VABCELspAavBYgatJDdbV1aX8/Hx1dnYqLy/P6XAAAEAEIv39plEeAABwFMUIAABwFF17ARfq6e3Xy42tOnbqrKZOmqC7KqYpK5O/PQCYiWIEcJnqrc2q2dWioW1Xntx6WMvnlKhqUalzgQFACBQjgItUb23WT3e2jFjeb2lgOQUJANNw3BZwiZ7eftXsGlmIDFWzq0U9vf1JiggAIkMxArjEy42tQadmRtNv+ccBgEkoRgCXOHbqrK3jACBZKEYAl5g6aYKt4wAgWShGAJe4q2KaxprlPMPjHwcAJqEYAVwiKzNDy+eUhB2zfE4J840AMA639gIuErhtd/g8IxkeMc8IAGPRKA9wIWZgBWCCSH+/OTICuFBWZobumXOJ02EAQET4UwkAADiKYgQAADiK0zTAEJ/19OmHW5vV+vFZTZs8QY8tKtX4LK/TYaWtvn5Lb7ecUsfpcyrIzdF1JZPkHev+ZQAph2IE+LPlG/ZoR3PHwPNdR6WXdx/XLaUFqll6rYORpadth9q0Zkuz2jrPDSzz5efoicWlWlDmczAyAHbjNA2gkYXIUDuaO7R8w54kR5Teth1q0321+4MKEUlq7zyn+2r3a9uhNociA5AIFCNIe5/19IUsRAJ2NHfos56+JEWU3vr6La3Z0qzR5hwILFuzpVl9Y3UFBJAyKEaQ9n64tdnWcYjP2y2nRhwRGcqS1NZ5Tm+3nEpeUAASimIEaa/148i62EY6DvHpOB26EIllHADzUYwg7U2bHFkX20jHIT4FuTm2jgNgPooRpL3HIuzXEuk4xOe6kkny5eco1A28HvnvqrmuZFIywwKQQBQjSHvjs7y6pbQg7JhbSguYbyRJvBkePbHYX/gNL0gCz59YXMp8I4CLUIwAkmqWXhuyIGGekeRbUObTujuvUVF+8KmYovwcrbvzGuYZAVyGrr3AEMzAahZmYAVSW6S/3xQjAAAgISL9/eY0DQAAcBTFCAAAcBSN8oAhTLhGwY4YTFgPAIgUxQjwZyZ0ibUjBhPWAwCiwWkaQGZ0ibUjBhPWAwCiRTGCtGdCl1g7YjBhPQAgFhQjSHsmdIm1IwYT1gMAYkExgrRnQpdYO2IwYT0AIBYUI0h7JnSJtSMGE9YDAGJBMYK0Z0KXWDtiMGE9ACAWFCNIeyZ0ibUjBhPWAwBiQTECyIwusXbEYMJ6AEC0aJQHDGHCzKXMwArALSL9/WYGVmAIb4ZHFZdOTvkYTFgPAIgUp2kAAICjKEYAAICjOE2T4tx0bUC86+KmXABAOqEYSWFu6s4a77q4KRcAkG44TZOi3NSdNd51cVMuACAdUYykIDd1Z413XdyUCwBIVxQjKchN3VnjXRc35QIA0hXFSApyU3fWeNfFTbkAgHRFMZKC3NSdNd51cVMuACBdUYykIDd1Z413XdyUCwBIVxQjKchN3VnjXRc35QIA0hXFSIpyU3fWeNfFTbkAgHRE194U56ZZR5mBFQDcha69acJN3VnjXRc35QIA0gmnaQAAgKMoRgAAgKM4TQPX6Ont18uNrTp26qymTpqguyqmKSszunrbjs9wy7UrblkPAOaL6gLWdevWad26dWptbZUkzZgxQ48//rgWLlw46vj6+nrddNNNI5YfPnxYV1xxRcRBcgErxlK9tVk1u1o0tAVNhkdaPqdEVYtKk/YZbuke7Jb1AOCsSH+/o/qT76KLLtJTTz2lvXv3au/evfrKV76iv/7rv9Y777wT9n1HjhxRW1vbwOPyyy+P5muBsKq3NuunO4OLCEnqt6Sf7mxR9dbmpHyGW7oHu2U9AKSOqIqRxYsXa9GiRfrCF76gL3zhC3ryySd1/vnna/fu3WHfV1BQoKKiooGH1+uNK2ggoKe3XzW7WsKOqdnVop7e/oR+hlu6B7tlPQCklpgvYO3r69PGjRv16aefqqKiIuzYmTNnyufzad68eXrzzTfH/Ozu7m51dXUFPYDRvNzYOuJoxnD9ln9cIj/DLd2D3bIeAFJL1MXIwYMHdf755ys7O1uVlZV65ZVXVFo6+vl0n8+n9evXq66uTps2bdL06dM1b9487dy5M+x3VFdXKz8/f+BRXFwcbZhIE8dOnY17nB2f4ZbuwW5ZDwCpJeq7aaZPn66mpib96U9/Ul1dne6++241NDSMWpBMnz5d06dPH3heUVGhEydO6JlnntHcuXNDfkdVVZVWrVo18Lyrq4uCBKOaOmlC3OPs+Ay3dA92y3oASC1RHxnJysrSZZddpvLyclVXV+vqq6/Wj3/844jff/311+vo0aNhx2RnZysvLy/oAYzmroppGutu0wyPf1wiP8Mt3YPdsh4AUkvck55ZlqXu7u6Ixx84cEA+H7cGwh5ZmRlaPqck7Jjlc0rCzhVix2e4pXuwW9YDQGqJ6jTNY489poULF6q4uFinT5/Wxo0bVV9fr23btknyn145efKkNmzYIElau3atpk2bphkzZqinp0e1tbWqq6tTXV2d/WuCtBWYAySeOULs+IxA9+Dh83MUpdj8HG5ZDwCpI6pi5IMPPtBdd92ltrY25efn66qrrtK2bdt0yy23SJLa2tp0/PjxgfE9PT165JFHdPLkSY0fP14zZszQa6+9pkWLFtm7Fkh7VYtK9e35V8Q1e6odn7GgzKdbSotSfuZSt6wHgNQQ1QysTmEGVgAAUk9CZmAFAACwG8UIAABwFF17U5wpnVXt6HZrQgx25NOEbWLC9sAQ/X3SsbekMx9I5xdKU2dLGbTFAAK4ZiSFmdJZ1Y5utybEYEc+TdgmJmwPDNG8Wdq2Wup6f3BZ3hRpwdNS6VediwtIgkh/vylGUlSgs+rwjRf4+3vdndck5ccv0O02lG/OTfwPoB0x2JFPE7aJCdsDQzRvlv5nqRRqr/i7DRQkcDUuYHUxUzqr2tHt1oQY7MinCdvEhO2BIfr7/EdEwu0V2x71jwPSHMVICjKls6od3W5NiMGOfJqwTUzYHhji2FvBp2ZGsKSuk/5xQJqjGElBpnRWtaPbrQkx2JFPE7aJCdsDQ5z5wN5xgItRjKQgUzqr2tHt1oQY7MinCdvEhO2BIc4vtHcc4GIUIynIlM6qdnS7NSEGO/JpwjYxYXtgiKmz/XfNhNsr8v6ffxyQ5ihGUpApnVXt6HZrQgx25NOEbWLC9sAQGV7/7buSQu4VC55ivhFAFCMpK9BZtSg/+LB/UX5O0m7rlfzN5b45t2TEX+QZnuTdRmpHDHbk04RtYsL2wBClX/Xfvps3bNvnTeG2XmAI5hlJcSbM9imZMeMnM7AOMmF7YAhmYEWaYtIzAADgKCY9AwAAKYFiBAAAOIquvXANt1zvARiLa1+QIBQjcAW3dNwFjEX3YSQQp2mQ8gLdcof3hmnvPKf7avdr26G2pHwG4FqB7sPDe+10tfmXN292Ji64BsUIUppbOu4CxqL7MJKAYgQpzS0ddwFj0X0YSUAxgpTmlo67gLHoPowkoBhBSnNLx13AWHQfRhJQjCCluaXjLmAsug8jCShGkNLc0nEXMBbdh5EEFCNIeW7puAsYi+7DSDAa5cE1mIEVSDBmYEWUIv39ZgZWuIY3w6OKSyc7/hmAa2V4pZI5TkcBF+I0DQAAcBTFCAAAcBSnaeJgwvUFdsTQ09uvlxtbdezUWU2dNEF3VUxTVmbq1akmbA+4FNdK2IdcmsWQ7UExEiMTOrzaEUP11mbV7GrR0LYrT249rOVzSlS1qNTukBPGhO0Bl6JbrX3IpVkM2h7cTRODQIfX4YkL/A2ejFtB7YihemuzfrqzJeTr35ybGgWJCdsDLhXoVhtq7+K21siRS7MkaXtE+vudesfiHWZCh1c7Yujp7VfNrtCFiCTV7GpRT29/7IEmgQnbAy5Ft1r7kEuzGLg9KEaiZEKHVztieLmxVWP9Pvdb/nEmM2F7wKXoVmsfcmkWA7cHxUiUTOjwakcMx06djegzIh3nFBO2B1yKbrX2IZdmMXB7UIxEyYQOr3bEMHXShIg+I9JxTjFhe8Cl6FZrH3JpFgO3B8VIlEzo8GpHDHdVTNNYd71mePzjTGbC9oBL0a3WPuTSLAZuD4qRKJnQ4dWOGLIyM7R8TknY71k+p8T4+UZM2B5wKbrV2odcmsXA7WH2L42hTOjwakcMVYtK9c25JSOOkGR4Uue2XsmM7QGXolutfcilWQzbHswzEgcTZvxkBtZBJmwPuJQhs1S6Ark0S4K3R6S/3xQjAAAgIZj0DAAApASKEQAA4Cga5aU4U66T4NoVAGmlt0faUyN90ipdME26drmUmZX8OFxyDQ7XjKQwUzrVJqp7cIZHKdc9GEAa2P49qfE5yRrSu8uTIVWslOZ/P3lxGNR1NxSuGXG5QKfa4X1Z2jvP6b7a/dp2qC1l4gh0Dx7eK6ffkn66s0XVW5vtDBkAYrf9e9JbzwYXIpL/+VvP+l9PhkDX3eE9Zrra/MubNycnDptQjKQgUzrV0j0YQFrp7fEfEQmn8Sf+cYlkYNfdeFGMpCBTOtXSPRhAWtlTM/KIyHBWn39cIhnYdTdeFCMpyJROtXQPBpBWPmm1d1ysDOy6Gy+KkRRkSqdaugcDSCsXTLN3XKwM7LobL4qRFGRKp1q6BwNIK9cu9981E47H6x+XSAZ23Y0XxUgKMqVTLd2DAaSVzCz/7bvhVKxI/HwjBnbdjRf/wqcoUzrV0j0YQFqZ/31p9oMjj5B4vP7lyZpnxLCuu/Fi0rMUxwysAOAAZmCNCF17AQCAo5iBFQAApASKEQAA4Ki07dprxzUOplyvYQI7rveIN59sjyFMOI9sxzl1E9bDlDhMiMFN4s0n28NWUV0zsm7dOq1bt06tra2SpBkzZujxxx/XwoULQ76noaFBq1at0jvvvKMpU6bon/7pn1RZWRlVkHZfM2JHl1lTOuaawI6Ou/Hmk+0xhAmdPO3oamrCepgShwkxuEm8+WR7RCwhF7Bu2bJFXq9Xl112mSTpZz/7mX70ox/pwIEDmjFjxojxLS0tKisr0/Lly/XNb35Tv/3tb3X//ffrF7/4he644w7bVyYSgS6zw1c68PdzJLej2vEZbhHouBtKJLfmxptPtscQgU6eobKRjFv+Al1NQ4nk9kcT1sOUOEyIwU3izSfbIypJu5tm0qRJ+tGPfqR77rlnxGurV6/W5s2bdfjw4YFllZWV+t3vfqfGxsaIv8OuYqSv39INT78RsrmbR/75MX6z+ishD+/b8Rlu0dPbryu+96uwje4yPNK7318Y8pRNvPlkewzR3yetLQvTQMvj/+vtoYOJO5zc2yM9WRi+mZjHK32nPfQpGxPWw5Q4TIjBTeLNJ9sjagm/m6avr08bN27Up59+qoqKilHHNDY2av78+UHLbr31Vu3du1eff/55yM/u7u5WV1dX0MMOdnSZNaVjrgns6Lgbbz7ZHkOY0MnTjq6mJqyHKXGYEIObxJtPtkfCRF2MHDx4UOeff76ys7NVWVmpV155RaWlox+Gb29vV2FhcKOewsJC9fb26qOPPgr5HdXV1crPzx94FBcXRxvmqOzoMmtKx1wT2NFxN958sj2GMKGTpx1dTU1YD1PiMCEGN4k3n2yPhIm6GJk+fbqampq0e/du3Xfffbr77rvV3NwccrzHE3xoPHBWaPjyoaqqqtTZ2TnwOHHiRLRhjsqOLrOmdMw1gR0dd+PNJ9tjCBM6edrR1dSE9TAlDhNicJN488n2SJioi5GsrCxddtllKi8vV3V1ta6++mr9+Mc/HnVsUVGR2tvbg5Z1dHQoMzNTkydPDvkd2dnZysvLC3rYwY4us6Z0zDWBHR13480n22MIEzp52tHV1IT1MCUOE2Jwk3jzyfZImLgnPbMsS93d3aO+VlFRoR07dgQt2759u8rLyzVu3Lh4vzpqdnSZNaVjrgns6Lgbbz7ZHkOY0MnTjq6mJqyHKXGYEIObxJtPtkfCRFWMPPbYY9q1a5daW1t18OBBfec731F9fb2WLFkiyX96ZenSpQPjKysrdezYMa1atUqHDx/Wiy++qBdeeEGPPPKIvWsRBTu6zJrSMdcEdnTcjTefbI8hTOjkaUdXUxPWw5Q4TIjBTeLNJ9sjIaK6tfeee+7R//3f/6mtrU35+fm66qqrtHr1at1yyy2SpGXLlqm1tVX19fUD72loaNDDDz88MOnZ6tWrHZ/0TGIGVrsxA6thTJgdkhlY3ReDmzADa1LQtRcAADiKrr0AACAlUIwAAABHpW3XXjtwjQIQhh3n1N1yXt6UXJiQTxNiMCkOSKIYiRldYoEw7Ohq6pbOqKbkwoR8mhCDSXFgABewxoAusUAYdnQ1dUtnVFNyYUI+TYjBpDjSBBewJkhfv6U1W5pH7MbS4K69Zkuz+sbqIAe4UX+f/y/OcP+HbHvUPy6Rn2ECU3JhQj5NiMGkODACxUiU6BILhGFHV1O3dEY1JRcm5NOEGEyKAyNQjESJLrFAGHZ0NXVLZ1RTcmFCPk2IwaQ4MALFSJToEguEYUdXU7d0RjUlFybk04QYTIoDI1CMRIkusUAYdnQ1dUtnVFNyYUI+TYjBpDgwAsVIlOgSC4RhR1dTt3RGNSUXJuTThBhMigMjUIzEgC6xQBh2dDV1S2dUU3JhQj5NiMGkOBCEeUbiwAysQBhumTHUDqbkwoR8mhCDSXG4HF17AQCAo5j0DAAApASKEQAA4Cga5QEwV2+PtKdG+qRVumCadO1yKTPL6aicQS4GueV6D7eshw24ZgSAmbZ/T2p8TrL6B5d5MqSKldL87zsXlxPIxSC3dNx1y3qMgWtGAKSu7d+T3no2+MdX8j9/61n/6+mCXAwKdNwd3l+mq82/vHmzM3FFyy3rYSOKEQBm6e3xHwUIp/En/nFuRy4GuaXjrlvWw2YUIwDMsqdm5FGA4aw+/zi3IxeD3NJx1y3rYTOKEQBm+aTV3nGpjFwMckvHXbesh80oRgCY5YJp9o5LZeRikFs67rplPWxGMQLALNcu998pEo7H6x/nduRikFs67rplPWxGMQLALJlZ/ltWw6lYkR5zbJCLQW7puOuW9bAZxQgA88z/vjT7wZFHBTxe//J0mluDXAxyS8ddt6yHjZj0DIC5mHV0ELkY5JaZS92yHmHQtRcAADiKGVgBAEBKoBgBAACOomsvYLc0OA8cEVPyYMK1FqbkAjAUxQhgpzTpxDkmU/IwWrfb7d9NbrdbU3IBGIzTNIBd6MTpZ0oeTOh2a0ouAMNRjAB2oBOnnyl5MKHbrSm5AFIAxQhgBzpx+pmSBxO63ZqSCyAFUIwAdqATp58peTCh260puQBSAMUIYAc6cfqZkgcTut2akgsgBVCMAHagE6efKXkwodutKbkAUgDFCGAHOnH6mZIHE7rdmpILIAVQjAB2oROnnyl5MKHbrSm5AAxHozzAbsy26WdKHpiBFXAMXXsBAICj6NoLAABSAsUIAABwFI3yAIzOhOsc7IjBhPUAEBbFCICRTOg0a0cMJqwHgDFxmgZAMBM6zdoRgwnrASAiFCMABpnQadaOGExYDwARoxgBMMiETrN2xGDCegCIGMUIgEEmdJq1IwYT1gNAxChGAAwyodOsHTGYsB4AIkYxAmCQCZ1m7YjBhPUAEDGKEQCDTOg0a0cMJqwHgIhRjAAIZkKnWTtiMGE9AESERnkARmfCzKXMwAqktEh/v5mBFcDoMrxSyZzUj8GE9QAQFqdpAACAoyhGAACAozhNAwzF9QWD4s0FuXQftikSJKpipLq6Wps2bdK7776r8ePHa/bs2Xr66ac1ffr0kO+pr6/XTTfdNGL54cOHdcUVV0QfMZAodHgdFG8uyKX7sE2RQFGdpmloaNCKFSu0e/du7dixQ729vZo/f74+/fTTMd975MgRtbW1DTwuv/zymIMGbEeH10Hx5oJcug/bFAkW1629H374oQoKCtTQ0KC5c+eOOiZwZOSTTz7RxIkTY/oebu1FQvX3SWvLwjRW8/j/AnzooPsPScebC3LpPmxTxCHS3++4LmDt7OyUJE2aNGnMsTNnzpTP59O8efP05ptvhh3b3d2trq6uoAeQMHR4HRRvLsil+7BNkQQxFyOWZWnVqlW64YYbVFZWFnKcz+fT+vXrVVdXp02bNmn69OmaN2+edu7cGfI91dXVys/PH3gUFxfHGiYwNjq8Doo3F+TSfdimSIKY76ZZuXKlfv/73+s3v/lN2HHTp08PusC1oqJCJ06c0DPPPBPy1E5VVZVWrVo18Lyrq4uCBIlDh9dB8eaCXLoP2xRJENORkQceeECbN2/Wm2++qYsuuijq919//fU6evRoyNezs7OVl5cX9AAShg6vg+LNBbl0H7YpkiCqYsSyLK1cuVKbNm3SG2+8oZKSkpi+9MCBA/L5fGMPBJKBDq+D4s0FuXQftimSIKpiZMWKFaqtrdXPf/5z5ebmqr29Xe3t7frss88GxlRVVWnp0qUDz9euXatXX31VR48e1TvvvKOqqirV1dVp5cqV9q0FEC86vA6KNxfk0n3YpkiwqG7t9XhGP0z30ksvadmyZZKkZcuWqbW1VfX19ZKkf/3Xf9X69et18uRJjR8/XjNmzFBVVZUWLVoUcZDc2oukYYbJQczAiuHYpohSpL/fcc0zkiwUIwAApJ6kzDMCAAAQL4oRAADgKLr2whycjzZLb4+0p0b6pFW6YJp07XIpM8vpqAC4EMUIzEBHULNs/57U+Jxk9Q9Z9l2pYqU0//vOxQXAlThNA+fREdQs278nvfVscCEi+Z+/9az/dQCwEcUInNXf5z8iotFu6vrzsm2P+sch8Xp7/EdEwmn8iX8cANiEYgTOoiOoWfbUjDwiMpzV5x8HADahGIGz6Ahqlk9a7R0HABGgGIGz6Ahqlgum2TsOACJAMQJn0RHULNculzxj/LPg8frHAYBNKEbgLDqCmiUzy3/7bjgVK5hvBICtKEbgPDqCmmX+96XZD448QuLx+pczzwgAm9EoD+ZgBlazMAMrgDhF+vvNDKwwR4ZXKpnjdBQIyMzyn5IBgATjNA0AAHAUxQgAAHAUp2kc1tdv6e2WU+o4fU4FuTm6rmSSvBmhbnNFWFxzYi/yieHYJ5AgFCMO2naoTWu2NKut89zAMl9+jp5YXKoFZb4w78QIdP21F/nEcOwTSCBO0zhk26E23Ve7P6gQkaT2znO6r3a/th1qcyiyFETXX3uRTwzHPoEEoxhxQF+/pTVbmsP1qdWaLc3q6zf+rmvn0fXXXuQTw7FPIAkoRhzwdsupEUdEhrIktXWe09stp5IXVKqi66+9yCeGY59AElCMOKDjdOhCJJZxaY2uv/YinxiOfQJJQDHigILcHFvHpTW6/tqLfGI49gkkAcWIA64rmSRffk64PrXy5ftv88UY6PprL/KJ4dgnkAQUIw7wZnj0xOJSSSH71OqJxaXMNxIJuv7ai3xiOPYJJAHFiEMWlPm07s5rVJQffCqmKD9H6+68hnlGokHXX3uRTwzHPoEEo2uvw5iB1UbMDmkv8onh2CcQpUh/vylGAABAQkT6+81pGgAA4CiKEQAA4Cga5QFAoplyrYUpcQDDUIwAQCKZ0u3WlDiAUXCaBgASxZRut6bEAYRAMQIAiWBKt1tT4gDCoBgBgEQwpdutKXEAYVCMAEAimNLt1pQ4gDAoRgAgEUzpdmtKHEAYFCMAkAimdLs1JQ4gDIoRAEgEU7rdmhIHEAbFCAAkiindbk2JAwiBRnkAkGimzHxqShxIG5H+fjMDKwAkWoZXKpnjdBTmxAEMw2kaAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgKIoRAADgqEynAwBs098nHXtLOvOBdH6hNHW2lOF1OioAwBiiOjJSXV2ta6+9Vrm5uSooKNDf/M3f6MiRI2O+r6GhQbNmzVJOTo4uueQSPf/88zEHDIyqebO0tkz62e1S3T3+/64t8y8HABgtqmKkoaFBK1as0O7du7Vjxw719vZq/vz5+vTTT0O+p6WlRYsWLdKcOXN04MABPfbYY3rwwQdVV1cXd/CAJH/B8T9Lpa73g5d3tfmXU5AAgNE8lmVZsb75ww8/VEFBgRoaGjR37txRx6xevVqbN2/W4cOHB5ZVVlbqd7/7nRobGyP6nq6uLuXn56uzs1N5eXmxhgs36u/zHwEZXogM8Eh5U6SHDnLKBgCSLNLf77guYO3s7JQkTZo0KeSYxsZGzZ8/P2jZrbfeqr179+rzzz8f9T3d3d3q6uoKegCjOvZWmEJEkiyp66R/HADASDEXI5ZladWqVbrhhhtUVlYWclx7e7sKCwuDlhUWFqq3t1cfffTRqO+prq5Wfn7+wKO4uDjWMOF2Zz6wdxwAIOliLkZWrlyp3//+9/rFL34x5liPxxP0PHBmaPjygKqqKnV2dg48Tpw4EWuYcLvzC8ceE804AEDSxXRr7wMPPKDNmzdr586duuiii8KOLSoqUnt7e9Cyjo4OZWZmavLkyaO+Jzs7W9nZ2bGEhnQzdbb/mpCuNkmjXf7052tGps5OdmQAgAhFdWTEsiytXLlSmzZt0htvvKGSkpIx31NRUaEdO3YELdu+fbvKy8s1bty46KIFhsvwSgue/vOT4Ufa/vx8wVNcvAoABouqGFmxYoVqa2v185//XLm5uWpvb1d7e7s+++yzgTFVVVVaunTpwPPKykodO3ZMq1at0uHDh/Xiiy/qhRde0COPPGLfWiC9lX5V+rsNUp4veHneFP/y0q86ExcAICJR3dob6hqPl156ScuWLZMkLVu2TK2traqvrx94vaGhQQ8//LDeeecdTZkyRatXr1ZlZWXEQXJrLyLCDKwAYJRIf7/jmmckWShGAABIPUmZZwQAACBeFCMAAMBRFCMAAMBRFCMAAMBRFCMAAMBRFCMAAMBRFCMAAMBRFCMAAMBRFCMAAMBRMXXtTbbAJLFdXV0ORwIAACIV+N0ea7L3lChGTp8+LUkqLi52OBIAABCt06dPKz8/P+TrKdGbpr+/X++//75yc3NDNutLZV1dXSouLtaJEyfovRMncmkv8mkfcmkv8mmfRObSsiydPn1aU6ZMUUZG6CtDUuLISEZGhi666CKnw0i4vLw8/qeyCbm0F/m0D7m0F/m0T6JyGe6ISAAXsAIAAEdRjAAAAEdRjBggOztbTzzxhLKzs50OJeWRS3uRT/uQS3uRT/uYkMuUuIAVAAC4F0dGAACAoyhGAACAoyhGAACAoyhGAACAoyhGkqi6uloej0cPPfRQyDH19fXyeDwjHu+++27yAjXUP//zP4/IS1FRUdj3NDQ0aNasWcrJydEll1yi559/PknRmi/afLJvhnfy5Endeeedmjx5siZMmKAvfvGL2rdvX9j3sH+GFm0+2T9HN23atFHzsmLFipDvcWK/TIkZWN1gz549Wr9+va666qqIxh85ciRoJrwLL7wwUaGllBkzZujXv/71wHOv1xtybEtLixYtWqTly5ertrZWv/3tb3X//ffrwgsv1B133JGMcI0XTT4D2DdH+uSTT/SlL31JN910k371q1+poKBAf/zjHzVx4sSQ72H/DC2WfAawfwbbs2eP+vr6Bp4fOnRIt9xyi772ta+NOt6p/ZJiJAnOnDmjJUuWqKamRj/4wQ8iek9BQUFE/+Olm8zMzDGPhgQ8//zzuvjii7V27VpJ0pVXXqm9e/fqmWeeSft/7AOiyWcA++ZITz/9tIqLi/XSSy8NLJs2bVrY97B/hhZLPgPYP4MNL8aeeuopXXrppbrxxhtHHe/UfslpmiRYsWKFbrvtNt18880Rv2fmzJny+XyaN2+e3nzzzQRGl1qOHj2qKVOmqKSkRH//93+v9957L+TYxsZGzZ8/P2jZrbfeqr179+rzzz9PdKgpIZp8BrBvjrR582aVl5fra1/7mgoKCjRz5kzV1NSEfQ/7Z2ix5DOA/TO0np4e1dbW6hvf+EbIprNO7ZcUIwm2ceNG7d+/X9XV1RGN9/l8Wr9+verq6rRp0yZNnz5d8+bN086dOxMcqfn+6q/+Shs2bNDrr7+umpoatbe3a/bs2fr4449HHd/e3q7CwsKgZYWFhert7dVHH32UjJCNFm0+2TdDe++997Ru3Tpdfvnlev3111VZWakHH3xQGzZsCPke9s/QYskn++fYXn31Vf3pT3/SsmXLQo5xbL+0kDDHjx+3CgoKrKampoFlN954o/Wtb30rqs+5/fbbrcWLF9scXeo7c+aMVVhYaP3bv/3bqK9ffvnl1g9/+MOgZb/5zW8sSVZbW1syQkwpY+VzNOybfuPGjbMqKiqClj3wwAPW9ddfH/I97J+hxZLP0bB/Bps/f751++23hx3j1H7JkZEE2rdvnzo6OjRr1ixlZmYqMzNTDQ0NevbZZ5WZmRl0UVE4119/vY4ePZrgaFPPeeedp7/8y78MmZuioiK1t7cHLevo6FBmZqYmT56cjBBTylj5HA37pp/P51NpaWnQsiuvvFLHjx8P+R72z9Biyedo2D8HHTt2TL/+9a917733hh3n1H5JMZJA8+bN08GDB9XU1DTwKC8v15IlS9TU1BTRnQuSdODAAfl8vgRHm3q6u7t1+PDhkLmpqKjQjh07gpZt375d5eXlGjduXDJCTClj5XM07Jt+X/rSl3TkyJGgZX/4wx80derUkO9h/wwtlnyOhv1z0EsvvaSCggLddtttYcc5tl8m7JgLRjX8NM2jjz5q3XXXXQPP/+M//sN65ZVXrD/84Q/WoUOHrEcffdSSZNXV1TkQrVm+/e1vW/X19dZ7771n7d6927r99tut3Nxcq7W11bKskbl87733rAkTJlgPP/yw1dzcbL3wwgvWuHHjrF/+8pdOrYJRos0n+2Zob7/9tpWZmWk9+eST1tGjR63/+q//siZMmGDV1tYOjGH/jFws+WT/DK2vr8+6+OKLrdWrV494zZT9kmIkyYYXI3fffbd14403Djx/+umnrUsvvdTKycmxLrjgAuuGG26wXnvtteQHaqCvf/3rls/ns8aNG2dNmTLF+tu//VvrnXfeGXh9eC4ty7Lq6+utmTNnWllZWda0adOsdevWJTlqc0WbT/bN8LZs2WKVlZVZ2dnZ1hVXXGGtX78+6HX2z+hEm0/2z9Bef/11S5J15MiREa+Zsl96LMuyEnfcBQAAIDyuGQEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI6iGAEAAI76/7Y16Pq3PgqBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:50,0],X[:50,1], label='0')\n",
    "plt.scatter(X[50:,0],X[50:,1], label='1')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### AdaBoost in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50, learning_rate=1.0):\n",
    "        self.clf_num = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def init_args(self, datasets, labels):\n",
    "\n",
    "        self.X = datasets\n",
    "        self.Y = labels\n",
    "        self.M, self.N = datasets.shape\n",
    "\n",
    "        # 弱分类器数目和集合\n",
    "        self.clf_sets = []\n",
    "\n",
    "        # 初始化weights\n",
    "        self.weights = [1.0 / self.M] * self.M\n",
    "\n",
    "        # G(x)系数 alpha\n",
    "        self.alpha = []\n",
    "\n",
    "    def _G(self, features, labels, weights):\n",
    "        m = len(features)\n",
    "        error = 100000.0  # 无穷大\n",
    "        best_v = 0.0\n",
    "        # 单维features\n",
    "        features_min = min(features)\n",
    "        features_max = max(features)\n",
    "        n_step = (features_max - features_min +\n",
    "                  self.learning_rate) // self.learning_rate\n",
    "        # print('n_step:{}'.format(n_step))\n",
    "        direct, compare_array = None, None\n",
    "        for i in range(1, int(n_step)):\n",
    "            v = features_min + self.learning_rate * i\n",
    "\n",
    "            if v not in features:\n",
    "                # 误分类计算\n",
    "                compare_array_positive = np.array(\n",
    "                    [1 if features[k] > v else -1 for k in range(m)])\n",
    "                weight_error_positive = sum([\n",
    "                    weights[k] for k in range(m)\n",
    "                    if compare_array_positive[k] != labels[k]\n",
    "                ])\n",
    "\n",
    "                compare_array_nagetive = np.array(\n",
    "                    [-1 if features[k] > v else 1 for k in range(m)])\n",
    "                weight_error_nagetive = sum([\n",
    "                    weights[k] for k in range(m)\n",
    "                    if compare_array_nagetive[k] != labels[k]\n",
    "                ])\n",
    "\n",
    "                if weight_error_positive < weight_error_nagetive:\n",
    "                    weight_error = weight_error_positive\n",
    "                    _compare_array = compare_array_positive\n",
    "                    direct = 'positive'\n",
    "                else:\n",
    "                    weight_error = weight_error_nagetive\n",
    "                    _compare_array = compare_array_nagetive\n",
    "                    direct = 'nagetive'\n",
    "\n",
    "                # print('v:{} error:{}'.format(v, weight_error))\n",
    "                if weight_error < error:\n",
    "                    error = weight_error\n",
    "                    compare_array = _compare_array\n",
    "                    best_v = v\n",
    "        return best_v, direct, error, compare_array\n",
    "\n",
    "    # 计算alpha\n",
    "    def _alpha(self, error):\n",
    "        return 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "    # 规范化因子\n",
    "    def _Z(self, weights, a, clf):\n",
    "        return sum([\n",
    "            weights[i] * np.exp(-1 * a * self.Y[i] * clf[i])\n",
    "            for i in range(self.M)\n",
    "        ])\n",
    "\n",
    "    # 权值更新\n",
    "    def _w(self, a, clf, Z):\n",
    "        for i in range(self.M):\n",
    "            self.weights[i] = self.weights[i] * np.exp(\n",
    "                -1 * a * self.Y[i] * clf[i]) / Z\n",
    "\n",
    "    # G(x)的线性组合\n",
    "    def _f(self, alpha, clf_sets):\n",
    "        pass\n",
    "\n",
    "    def G(self, x, v, direct):\n",
    "        if direct == 'positive':\n",
    "            return 1 if x > v else -1\n",
    "        else:\n",
    "            return -1 if x > v else 1\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.init_args(X, y)\n",
    "\n",
    "        for epoch in range(self.clf_num):\n",
    "            best_clf_error, best_v, clf_result = 100000, None, None\n",
    "            # 根据特征维度, 选择误差最小的\n",
    "            for j in range(self.N):\n",
    "                features = self.X[:, j]\n",
    "                # 分类阈值，分类误差，分类结果\n",
    "                v, direct, error, compare_array = self._G(\n",
    "                    features, self.Y, self.weights)\n",
    "\n",
    "                if error < best_clf_error:\n",
    "                    best_clf_error = error\n",
    "                    best_v = v\n",
    "                    final_direct = direct\n",
    "                    clf_result = compare_array\n",
    "                    axis = j\n",
    "\n",
    "                # print('epoch:{}/{} feature:{} error:{} v:{}'.format(epoch, self.clf_num, j, error, best_v))\n",
    "                if best_clf_error == 0:\n",
    "                    break\n",
    "\n",
    "            # 计算G(x)系数a\n",
    "            a = self._alpha(best_clf_error)\n",
    "            self.alpha.append(a)\n",
    "            # 记录分类器\n",
    "            self.clf_sets.append((axis, best_v, final_direct))\n",
    "            # 规范化因子\n",
    "            Z = self._Z(self.weights, a, clf_result)\n",
    "            # 权值更新\n",
    "            self._w(a, clf_result, Z)\n",
    "\n",
    "\n",
    "#             print('classifier:{}/{} error:{:.3f} v:{} direct:{} a:{:.5f}'.format(epoch+1, self.clf_num, error, best_v, final_direct, a))\n",
    "#             print('weight:{}'.format(self.weights))\n",
    "#             print('\\n')\n",
    "\n",
    "    def predict(self, feature):\n",
    "        result = 0.0\n",
    "        for i in range(len(self.clf_sets)):\n",
    "            axis, clf_v, direct = self.clf_sets[i]\n",
    "            f_input = feature[axis]\n",
    "            result += self.alpha[i] * self.G(f_input, clf_v, direct)\n",
    "        # sign\n",
    "        return 1 if result > 0 else -1\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        right_count = 0\n",
    "        for i in range(len(X_test)):\n",
    "            feature = X_test[i]\n",
    "            if self.predict(feature) == y_test[i]:\n",
    "                right_count += 1\n",
    "\n",
    "        return right_count / len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(10).reshape(10, 1)\n",
    "y = np.array([1, 1, 1, -1, -1, -1, 1, 1, 1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoost(n_estimators=3, learning_rate=0.5)\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5151515151515151"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoost(n_estimators=10, learning_rate=0.2)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average score:63.394%\n"
     ]
    }
   ],
   "source": [
    "# 100次结果\n",
    "result = []\n",
    "for i in range(1, 101):\n",
    "    X, y = create_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "    clf = AdaBoost(n_estimators=100, learning_rate=0.2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    r = clf.score(X_test, y_test)\n",
    "    # print('{}/100 score：{}'.format(i, r))\n",
    "    result.append(r)\n",
    "\n",
    "print('average score:{:.3f}%'.format(sum(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn实例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "#### sklearn.ensemble.AdaBoostClassifier\n",
    "\n",
    "- algorithm：这个参数只有AdaBoostClassifier有。主要原因是scikit-learn实现了两种Adaboost分类算法，SAMME和SAMME.R。两者的主要区别是弱学习器权重的度量，SAMME使用了和我们的原理篇里二元分类Adaboost算法的扩展，即用对样本集分类效果作为弱学习器权重，而SAMME.R使用了对样本集分类的预测概率大小来作为弱学习器权重。由于SAMME.R使用了概率度量的连续值，迭代一般比SAMME快，因此AdaBoostClassifier的默认算法algorithm的值也是SAMME.R。我们一般使用默认的SAMME.R就够了，但是要注意的是使用了SAMME.R， 则弱分类学习器参数base_estimator必须限制使用支持概率预测的分类器。SAMME算法则没有这个限制。\n",
    "\n",
    "\n",
    "- n_estimators： AdaBoostClassifier和AdaBoostRegressor都有，就是我们的弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是50。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。\n",
    "\n",
    "\n",
    "-  learning_rate:  AdaBoostClassifier和AdaBoostRegressor都有，即每个弱学习器的权重缩减系数ν\n",
    "\n",
    "\n",
    "- base_estimator：AdaBoostClassifier和AdaBoostRegressor都有，即我们的弱分类学习器或者弱回归学习器。理论上可以选择任何一个分类或者回归学习器，不过需要支持样本权重。我们常用的一般是CART决策树或者神经网络MLP。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(learning_rate=0.5, n_estimators=100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=0.5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第8章提升方法-习题\n",
    "\n",
    "### 习题8.1\n",
    "&emsp;&emsp;某公司招聘职员考查身体、业务能力、发展潜力这3项。身体分为合格1、不合格0两级，业务能力和发展潜力分为上1、中2、下3三级分类为合格1 、不合格-1两类。已知10个人的数据，如下表所示。假设弱分类器为决策树桩。试用AdaBoost算法学习一个强分类器。  \n",
    "\n",
    "应聘人员情况数据表\n",
    "\n",
    "&emsp;&emsp;|1|2|3|4|5|6|7|8|9|10\n",
    "-|-|-|-|-|-|-|-|-|-|-\n",
    "身体|0|0|1|1|1|0|1|1|1|0\n",
    "业务|1|3|2|1|2|1|1|1|3|2\n",
    "潜力|3|1|2|3|3|2|2|1|1|1\n",
    "分类|-1|-1|-1|-1|-1|-1|1|1|-1|-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 加载训练数据\n",
    "X = np.array([[0, 1, 3], [0, 3, 1], [1, 2, 2], [1, 1, 3], [1, 2, 3], [0, 1, 2],\n",
    "              [1, 1, 2], [1, 1, 1], [1, 3, 1], [0, 2, 1]])\n",
    "y = np.array([-1, -1, -1, -1, -1, -1, 1, 1, -1, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AdaBoostClassifier分类器实现：**\n",
    "\n",
    "采用sklearn的AdaBoostClassifier分类器直接求解，由于AdaBoostClassifier分类器默认采用CART决策树弱分类器，故不需要设置base_estimator参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测正确率：100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(X, y)\n",
    "y_predict = clf.predict(X)\n",
    "score = clf.score(X, y)\n",
    "print(\"原始输出:\", y)\n",
    "print(\"预测输出:\", y_predict)\n",
    "print(\"预测正确率：{:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**自编程实现：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自编程求解例8.1\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, X, y, tol=0.05, max_iter=10):\n",
    "        # 训练数据 实例\n",
    "        self.X = X\n",
    "        # 训练数据 标签\n",
    "        self.y = y\n",
    "        # 训练中止条件 right_rate>self.tol\n",
    "        self.tol = tol\n",
    "        # 最大迭代次数\n",
    "        self.max_iter = max_iter\n",
    "        # 初始化样本权重w\n",
    "        self.w = np.full((X.shape[0]), 1 / X.shape[0])\n",
    "        self.G = []  # 弱分类器\n",
    "\n",
    "    def build_stump(self):\n",
    "        \"\"\"\n",
    "        以带权重的分类误差最小为目标，选择最佳分类阈值\n",
    "        best_stump['dim'] 合适的特征所在维度\n",
    "        best_stump['thresh']  合适特征的阈值\n",
    "        best_stump['ineq']  树桩分类的标识lt,rt\n",
    "        \"\"\"\n",
    "        m, n = np.shape(self.X)\n",
    "        # 分类误差\n",
    "        e_min = np.inf\n",
    "        # 小于分类阈值的样本属于的标签类别\n",
    "        sign = None\n",
    "        # 最优分类树桩\n",
    "        best_stump = {}\n",
    "        for i in range(n):\n",
    "            range_min = self.X[:, i].min()  # 求每一种特征的最大最小值\n",
    "            range_max = self.X[:, i].max()\n",
    "            step_size = (range_max - range_min) / n\n",
    "            for j in range(-1, int(n) + 1):\n",
    "                thresh_val = range_min + j * step_size\n",
    "                # 计算左子树和右子树的误差\n",
    "                for inequal in ['lt', 'rt']:\n",
    "                    predict_vals = self.base_estimator(self.X, i, thresh_val,\n",
    "                                                       inequal)\n",
    "                    err_arr = np.array(np.ones(m))\n",
    "                    err_arr[predict_vals.T == self.y.T] = 0\n",
    "                    weighted_error = np.dot(self.w, err_arr)\n",
    "                    if weighted_error < e_min:\n",
    "                        e_min = weighted_error\n",
    "                        sign = predict_vals\n",
    "                        best_stump['dim'] = i\n",
    "                        best_stump['thresh'] = thresh_val\n",
    "                        best_stump['ineq'] = inequal\n",
    "        return best_stump, sign, e_min\n",
    "\n",
    "    def updata_w(self, alpha, predict):\n",
    "        \"\"\"\n",
    "        更新样本权重w\n",
    "        \"\"\"\n",
    "        # 以下2行根据公式8.4 8.5 更新样本权重\n",
    "        P = self.w * np.exp(-alpha * self.y * predict)\n",
    "        self.w = P / P.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def base_estimator(X, dimen, threshVal, threshIneq):\n",
    "        \"\"\"\n",
    "        计算单个弱分类器（决策树桩）预测输出\n",
    "        \"\"\"\n",
    "        ret_array = np.ones(np.shape(X)[0])  # 预测矩阵\n",
    "        # 左叶子 ，整个矩阵的样本进行比较赋值\n",
    "        if threshIneq == 'lt':\n",
    "            ret_array[X[:, dimen] <= threshVal] = -1.0\n",
    "        else:\n",
    "            ret_array[X[:, dimen] > threshVal] = -1.0\n",
    "        return ret_array\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        对训练数据进行学习\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        for i in range(self.max_iter):\n",
    "            best_stump, sign, error = self.build_stump()  # 获取当前迭代最佳分类阈值\n",
    "            alpha = 1 / 2 * np.log((1 - error) / error)  # 计算本轮弱分类器的系数\n",
    "            # 弱分类器权重\n",
    "            best_stump['alpha'] = alpha\n",
    "            # 保存弱分类器\n",
    "            self.G.append(best_stump)\n",
    "            # 以下3行计算当前总分类器（之前所有弱分类器加权和）分类效率\n",
    "            G += alpha * sign\n",
    "            y_predict = np.sign(G)\n",
    "            error_rate = np.sum(\n",
    "                np.abs(y_predict - self.y)) / 2 / self.y.shape[0]\n",
    "            if error_rate < self.tol:  # 满足中止条件 则跳出循环\n",
    "                print(\"迭代次数:\", i + 1)\n",
    "                break\n",
    "            else:\n",
    "                self.updata_w(alpha, y_predict)  # 若不满足，更新权重，继续迭代\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        对新数据进行预测\n",
    "        \"\"\"\n",
    "        m = np.shape(X)[0]\n",
    "        G = np.zeros(m)\n",
    "        for i in range(len(self.G)):\n",
    "            stump = self.G[i]\n",
    "            # 遍历每一个弱分类器，进行加权\n",
    "            _G = self.base_estimator(X, stump['dim'], stump['thresh'],\n",
    "                                     stump['ineq'])\n",
    "            alpha = stump['alpha']\n",
    "            G += alpha * _G\n",
    "        y_predict = np.sign(G)\n",
    "        return y_predict.astype(int)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"对训练效果进行评价\"\"\"\n",
    "        y_predict = self.predict(X)\n",
    "        error_rate = np.sum(np.abs(y_predict - y)) / 2 / y.shape[0]\n",
    "        return 1 - error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "迭代次数: 8\n",
      "原始输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测输出: [-1 -1 -1 -1 -1 -1  1  1 -1 -1]\n",
      "预测正确率：100.00%\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoost(X, y)\n",
    "clf.fit()\n",
    "y_predict = clf.predict(X)\n",
    "score = clf.score(X, y)\n",
    "print(\"原始输出:\", y)\n",
    "print(\"预测输出:\", y_predict)\n",
    "print(\"预测正确率：{:.2%}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 习题8.2\n",
    "&emsp;&emsp;比较支持向量机、 AdaBoost 、Logistic回归模型的学习策略与算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**解答：**  \n",
    "- **支持向量机**  \n",
    "学习策略：极小化正则化合页损失，软间隔最大化；  \n",
    "学习算法：序列最小最优化算法（SMO）  \n",
    "- **AdaBoost**  \n",
    "学习策略：极小化加法模型指数损失；  \n",
    "学习算法：前向分步加法算法  \n",
    "- **Logistic回归**  \n",
    "学习策略：极大似然估计，正则化的极大似然估计；  \n",
    "学习算法：改进的迭代尺度算法，梯度下降，拟牛顿法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "参考代码：https://github.com/wzyonggege/statistical-learning-method\n",
    "\n",
    "本文代码更新地址：https://github.com/fengdu78/lihang-code\n",
    "\n",
    "习题解答：https://github.com/datawhalechina/statistical-learning-method-solutions-manual\n",
    "\n",
    "中文注释制作：机器学习初学者公众号：ID:ai-start-com\n",
    "\n",
    "配置环境：python 3.5+\n",
    "\n",
    "代码全部测试通过。\n",
    "![gongzhong](../gongzhong.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
